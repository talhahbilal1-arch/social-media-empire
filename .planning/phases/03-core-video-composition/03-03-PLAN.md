---
phase: 03-core-video-composition
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/video/timing.py
autonomous: true

must_haves:
  truths:
    - "Word boundary timing from edge-tts is extracted correctly"
    - "Words are grouped into sentence blocks with aggregated timing"
    - "Sentence timing has start time and duration accurate to audio"
  artifacts:
    - path: "src/video/timing.py"
      provides: "Audio-text synchronization utilities"
      exports: ["extract_word_timings", "group_words_into_sentences", "SentenceTiming"]
      min_lines: 70
  key_links:
    - from: "src/video/timing.py"
      to: "edge_tts.SubMaker"
      via: "SubMaker cues parsing"
      pattern: "submaker\\.cues"
---

<objective>
Create the audio-text synchronization system that converts edge-tts word boundaries into sentence-level timing blocks.

Purpose: Text overlays need to appear in sync with the voiceover. Edge-tts provides word-level timing, but we display sentence blocks per the project requirements. This module bridges that gap by grouping words into sentences while preserving accurate timing.

Output: `src/video/timing.py` with functions to extract timing from edge-tts SubMaker and group into sentence blocks.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-video-composition/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create timing module with data structures</name>
  <files>src/video/timing.py</files>
  <action>
Create `src/video/timing.py` with:

1. Imports:
```python
from dataclasses import dataclass
from typing import List
import edge_tts
```

2. WordTiming dataclass:
```python
@dataclass
class WordTiming:
    """Timing information for a single word from TTS."""
    text: str
    start: float  # seconds
    end: float    # seconds

    @property
    def duration(self) -> float:
        """Duration in seconds."""
        return self.end - self.start
```

3. SentenceTiming dataclass:
```python
@dataclass
class SentenceTiming:
    """Timing information for a sentence block."""
    text: str
    start: float    # seconds - when sentence starts in audio
    duration: float # seconds - how long to display

    @property
    def end(self) -> float:
        """End time in seconds."""
        return self.start + self.duration
```

These dataclasses provide type-safe timing data that the text overlay system will use.
  </action>
  <verify>
```bash
python -c "
from src.video.timing import WordTiming, SentenceTiming

word = WordTiming(text='Hello', start=0.0, end=0.5)
print(f'Word duration: {word.duration}')

sentence = SentenceTiming(text='Hello world', start=0.0, duration=1.5)
print(f'Sentence end: {sentence.end}')
print('Data structures work')
"
```
  </verify>
  <done>timing.py exists with WordTiming and SentenceTiming dataclasses</done>
</task>

<task type="auto">
  <name>Task 2: Implement extract_word_timings function</name>
  <files>src/video/timing.py</files>
  <action>
Add function to extract word timings from edge-tts SubMaker:

```python
def extract_word_timings(submaker: edge_tts.SubMaker) -> List[WordTiming]:
    """Extract word-level timing from edge-tts SubMaker.

    Args:
        submaker: SubMaker instance that has been fed WordBoundary events

    Returns:
        List of WordTiming objects with start/end in seconds

    Note:
        SubMaker.cues contain timedelta objects for start/end.
        We convert to float seconds for consistency.
    """
    word_timings = []

    for cue in submaker.cues:
        word_timings.append(WordTiming(
            text=cue.text,
            start=cue.start.total_seconds(),
            end=cue.end.total_seconds()
        ))

    return word_timings
```

The edge-tts SubMaker stores cues with:
- `cue.text` - the word
- `cue.start` - timedelta object
- `cue.end` - timedelta object

We convert timedelta to seconds using `.total_seconds()`.
  </action>
  <verify>
```bash
python -c "
from src.video.timing import extract_word_timings
import edge_tts

# Create a mock SubMaker to test the function signature
submaker = edge_tts.SubMaker()
# Without feeding data, cues will be empty - that's fine for signature test
timings = extract_word_timings(submaker)
print(f'extract_word_timings returns: {type(timings).__name__}')
print(f'Empty submaker gives empty list: {timings == []}')
"
```
  </verify>
  <done>extract_word_timings function parses SubMaker cues into WordTiming list</done>
</task>

<task type="auto">
  <name>Task 3: Implement group_words_into_sentences function</name>
  <files>src/video/timing.py</files>
  <action>
Add function to group word timings into sentence blocks:

```python
def group_words_into_sentences(
    word_timings: List[WordTiming],
    sentences: List[str]
) -> List[SentenceTiming]:
    """Group word timings into sentence-level timing blocks.

    Args:
        word_timings: List of word timings from extract_word_timings
        sentences: List of sentences (in order they appear in script)

    Returns:
        List of SentenceTiming objects matching the sentence order

    Algorithm:
        For each sentence, count its words and consume that many word timings.
        The sentence's start time is the first word's start.
        The sentence's duration spans from first word start to last word end.

    Note:
        This assumes sentences were split by punctuation and words match
        the TTS output order exactly. Minor word count mismatches are handled
        by using available words.
    """
    sentence_timings = []
    word_index = 0

    for sentence in sentences:
        # Count words in this sentence
        sentence_words = sentence.split()
        num_words = len(sentence_words)

        # Check if we have enough words remaining
        if word_index >= len(word_timings):
            break  # No more timing data available

        # Calculate how many words we can actually use
        available_words = min(num_words, len(word_timings) - word_index)

        if available_words > 0:
            # Get timing boundaries for this sentence
            start_time = word_timings[word_index].start
            end_time = word_timings[word_index + available_words - 1].end
            duration = end_time - start_time

            sentence_timings.append(SentenceTiming(
                text=sentence,
                start=start_time,
                duration=duration
            ))

            word_index += available_words

    return sentence_timings
```

This implementation:
1. Matches words by count (assumes TTS words align with sentence words)
2. Handles edge cases where word counts don't match perfectly
3. Preserves timing accuracy from the TTS word boundaries
  </action>
  <verify>
```bash
python -c "
from src.video.timing import group_words_into_sentences, WordTiming, SentenceTiming

# Create test word timings
words = [
    WordTiming('This', 0.0, 0.3),
    WordTiming('is', 0.3, 0.4),
    WordTiming('a', 0.4, 0.5),
    WordTiming('test.', 0.5, 0.8),
    WordTiming('It', 1.0, 1.1),
    WordTiming('works!', 1.1, 1.5),
]

sentences = ['This is a test.', 'It works!']
timings = group_words_into_sentences(words, sentences)

print(f'Got {len(timings)} sentence timings')
for st in timings:
    print(f'  \"{st.text}\" start={st.start:.1f}s duration={st.duration:.1f}s')

# Verify timing accuracy
assert len(timings) == 2
assert timings[0].start == 0.0
assert timings[0].duration == 0.8  # From 0.0 to 0.8
assert timings[1].start == 1.0
print('Timing accuracy verified')
"
```
  </verify>
  <done>group_words_into_sentences correctly aggregates word timings into sentence blocks</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. All exports work:
```bash
python -c "
from src.video.timing import (
    WordTiming,
    SentenceTiming,
    extract_word_timings,
    group_words_into_sentences
)
print('All exports available')
"
```

2. End-to-end timing flow:
```bash
python -c "
from src.video.timing import WordTiming, SentenceTiming, group_words_into_sentences

# Simulate a realistic scenario
words = [
    WordTiming('Menopause', 0.0, 0.6),
    WordTiming('can', 0.7, 0.8),
    WordTiming('be', 0.9, 1.0),
    WordTiming('challenging.', 1.1, 1.8),
    WordTiming('But', 2.0, 2.2),
    WordTiming('you', 2.3, 2.4),
    WordTiming('are', 2.5, 2.6),
    WordTiming('not', 2.7, 2.9),
    WordTiming('alone.', 3.0, 3.5),
]

sentences = ['Menopause can be challenging.', 'But you are not alone.']
timings = group_words_into_sentences(words, sentences)

# Verify sentence timing spans word boundaries correctly
assert timings[0].start == 0.0
assert timings[0].end == 1.8  # End of 'challenging.'
assert timings[1].start == 2.0
assert timings[1].end == 3.5  # End of 'alone.'
print('Realistic timing test passed')
"
```
</verification>

<success_criteria>
- src/video/timing.py exists with all required functions and dataclasses
- WordTiming dataclass has text, start, end, and duration property
- SentenceTiming dataclass has text, start, duration, and end property
- extract_word_timings parses edge_tts.SubMaker cues correctly
- group_words_into_sentences aggregates word timings into sentence blocks
- Timing accuracy is preserved (start/end from actual word boundaries)
- Edge cases handled (mismatched word counts, empty inputs)
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-video-composition/03-03-SUMMARY.md`
</output>
