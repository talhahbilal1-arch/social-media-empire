---
phase: 03-core-video-composition
plan: 04
type: execute
wave: 2
depends_on: ["03-01", "03-02", "03-03"]
files_modified:
  - src/video/compositor.py
  - src/video/__init__.py
  - scripts/test_compositor.py
autonomous: false

must_haves:
  truths:
    - "Compositor combines background video, text overlays, and audio into final MP4"
    - "Audio plays continuously without drift throughout video duration"
    - "Memory is released after each video generation (verified across 10 iterations)"
    - "Output is 1080x1920 MP4 with synced text overlays"
  artifacts:
    - path: "src/video/compositor.py"
      provides: "Complete VideoCompositor with compose_video method"
      exports: ["VideoCompositor"]
      contains: "compose_video"
    - path: "scripts/test_compositor.py"
      provides: "Integration test for video composition pipeline"
      min_lines: 100
  key_links:
    - from: "src/video/compositor.py"
      to: "src/video/text_overlay.py"
      via: "create_text_overlay import"
      pattern: "from src\\.video\\.text_overlay import"
    - from: "src/video/compositor.py"
      to: "moviepy CompositeVideoClip"
      via: "CompositeVideoClip import"
      pattern: "from moviepy import.*CompositeVideoClip"
    - from: "src/video/compositor.py"
      to: "AudioFileClip"
      via: "Audio loading"
      pattern: "AudioFileClip"
---

<objective>
Complete the video composition pipeline by wiring together the converter, text overlays, and audio into a final composited MP4.

Purpose: This is the integration plan that brings all components together. It implements the compose_video method that takes a stock video, audio file, and sentence timings to produce the final vertical video with synced text overlays.

Output: Complete VideoCompositor class with compose_video method, and verification script to test all success criteria including memory cleanup across 10 iterations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-video-composition/03-RESEARCH.md
@.planning/phases/03-core-video-composition/03-01-SUMMARY.md
@.planning/phases/03-core-video-composition/03-02-SUMMARY.md
@.planning/phases/03-core-video-composition/03-03-SUMMARY.md
@src/video/compositor.py
@src/video/text_overlay.py
@src/video/timing.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add compose_video method to VideoCompositor</name>
  <files>src/video/compositor.py</files>
  <action>
Update compositor.py to add the compose_video method:

1. Add imports at top of file:
```python
from moviepy import VideoFileClip, AudioFileClip, CompositeVideoClip
from src.video.text_overlay import create_text_overlay
from src.video.timing import SentenceTiming
from typing import List
```

2. Add compose_video method to VideoCompositor class:

```python
def compose_video(
    self,
    video_path: str,
    audio_path: str,
    sentence_timings: List[SentenceTiming],
    output_path: str,
    text_position: str = "center"
) -> None:
    """Compose final video with background, text overlays, and audio.

    Args:
        video_path: Path to stock video (any aspect ratio)
        audio_path: Path to TTS audio file
        sentence_timings: List of SentenceTiming from timing module
        output_path: Where to write the final MP4
        text_position: Default position for text ("top", "center", "bottom")

    Raises:
        FileNotFoundError: If video or audio files don't exist
        RuntimeError: If composition fails

    Note:
        Call cleanup() after this method completes to release memory.
    """
    # Convert stock video to vertical
    bg_clip = self.convert_to_vertical(video_path)

    # Load audio
    audio_clip = AudioFileClip(audio_path)
    self.clips_to_close.append(audio_clip)

    # Match video duration exactly to audio to prevent drift
    bg_clip = bg_clip.with_duration(audio_clip.duration)

    # Create text overlays for each sentence
    text_clips = []
    for timing in sentence_timings:
        txt_clip = create_text_overlay(
            text=timing.text,
            start_time=timing.start,
            duration=timing.duration,
            brand_config=self.brand_config,
            position=text_position
        )
        self.clips_to_close.append(txt_clip)
        text_clips.append(txt_clip)

    # Compose all layers - background first, then text on top
    video = CompositeVideoClip([bg_clip] + text_clips)
    video = video.with_audio(audio_clip)
    self.clips_to_close.append(video)

    # Export with optimized settings
    video.write_videofile(
        output_path,
        codec='libx264',
        audio_codec='aac',
        fps=TARGET_FPS,
        preset='medium',
        ffmpeg_params=['-crf', '23'],
        threads=4,
        logger=None  # Suppress progress bar in production
    )
```

CRITICAL points from research:
- Set video duration to match audio exactly: `bg_clip.with_duration(audio_clip.duration)`
- Use `CompositeVideoClip` for layering
- Track all clips in self.clips_to_close
- Use TARGET_FPS constant for consistent frame rate
  </action>
  <verify>
```bash
python -c "
from src.video.compositor import VideoCompositor
import inspect

# Check method exists and has correct signature
sig = inspect.signature(VideoCompositor.compose_video)
params = list(sig.parameters.keys())
expected = ['self', 'video_path', 'audio_path', 'sentence_timings', 'output_path', 'text_position']
assert params == expected, f'Got params: {params}'
print('compose_video method signature correct')
"
```
  </verify>
  <done>compose_video method exists with correct signature and implementation</done>
</task>

<task type="auto">
  <name>Task 2: Update video module exports</name>
  <files>src/video/__init__.py</files>
  <action>
Update `src/video/__init__.py` to export all components:

```python
"""Video composition module for social media vertical videos.

This module provides:
- VideoCompositor: Main class for compositing videos
- create_text_overlay: Factory for creating text clips
- TextOverlayConfig: Configuration for text styling
- Timing utilities: For audio-text synchronization
"""

from src.video.compositor import VideoCompositor
from src.video.text_overlay import create_text_overlay, TextOverlayConfig
from src.video.timing import (
    WordTiming,
    SentenceTiming,
    extract_word_timings,
    group_words_into_sentences,
)

__all__ = [
    "VideoCompositor",
    "create_text_overlay",
    "TextOverlayConfig",
    "WordTiming",
    "SentenceTiming",
    "extract_word_timings",
    "group_words_into_sentences",
]
```
  </action>
  <verify>
```bash
python -c "
from src.video import (
    VideoCompositor,
    create_text_overlay,
    TextOverlayConfig,
    WordTiming,
    SentenceTiming,
    extract_word_timings,
    group_words_into_sentences,
)
print('All exports available from src.video')
"
```
  </verify>
  <done>src/video/__init__.py exports all video module components</done>
</task>

<task type="auto">
  <name>Task 3: Create compositor verification script</name>
  <files>scripts/test_compositor.py</files>
  <action>
Create `scripts/test_compositor.py` to verify all Phase 3 success criteria:

```python
#!/usr/bin/env python3
"""Verification script for Phase 3: Core Video Composition.

Tests all success criteria:
1. 16:9 to 9:16 conversion without black bars
2. Text overlays with brand colors in safe zones
3. Text-audio sync within 100ms accuracy
4. Audio plays without drift
5. Memory cleanup across 10 consecutive generations

Usage:
    python scripts/test_compositor.py
"""

import asyncio
import gc
import os
import subprocess
import tempfile
import tracemalloc
from pathlib import Path

import edge_tts
from moviepy import VideoFileClip, ColorClip

from src.models.brand import BrandConfig, ColorPalette
from src.utils.brand_loader import load_brand
from src.video import (
    VideoCompositor,
    SentenceTiming,
    extract_word_timings,
    group_words_into_sentences,
)


def create_test_video(output_path: str, duration: float = 5.0) -> None:
    """Create a simple 16:9 test video (colored background)."""
    clip = ColorClip(size=(1920, 1080), color=(100, 100, 100), duration=duration)
    clip = clip.with_fps(24)
    clip.write_videofile(
        output_path,
        codec='libx264',
        fps=24,
        logger=None
    )
    clip.close()


async def create_test_audio(text: str, output_path: str, voice: str = "en-US-JennyNeural"):
    """Create TTS audio and return word timings."""
    communicate = edge_tts.Communicate(text, voice)
    submaker = edge_tts.SubMaker()

    audio_data = bytearray()
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_data.extend(chunk["data"])
        elif chunk["type"] == "WordBoundary":
            submaker.feed(chunk)

    Path(output_path).write_bytes(audio_data)
    return extract_word_timings(submaker)


def verify_video_dimensions(video_path: str) -> bool:
    """Verify output video is 1080x1920."""
    clip = VideoFileClip(video_path)
    result = clip.w == 1080 and clip.h == 1920
    clip.close()
    return result


def verify_audio_duration(video_path: str, audio_path: str, tolerance: float = 0.1) -> bool:
    """Verify video duration matches audio duration within tolerance."""
    video_clip = VideoFileClip(video_path)
    # Use ffprobe to get audio duration for accuracy
    result = subprocess.run(
        ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
         '-of', 'csv=p=0', audio_path],
        capture_output=True, text=True
    )
    audio_duration = float(result.stdout.strip())
    video_duration = video_clip.duration
    video_clip.close()

    diff = abs(video_duration - audio_duration)
    return diff <= tolerance


def test_single_video_generation(brand: BrandConfig, tmpdir: str) -> bool:
    """Test a single video generation cycle."""
    test_video_path = os.path.join(tmpdir, "test_input.mp4")
    audio_path = os.path.join(tmpdir, "test_audio.mp3")
    output_path = os.path.join(tmpdir, "test_output.mp4")

    # Create test inputs
    create_test_video(test_video_path, duration=10.0)

    # Create audio with timing
    text = "This is a test. It demonstrates video composition."
    sentences = ["This is a test.", "It demonstrates video composition."]

    word_timings = asyncio.run(create_test_audio(text, audio_path, brand.tts_voice))
    sentence_timings = group_words_into_sentences(word_timings, sentences)

    # Generate video
    compositor = VideoCompositor(brand)
    try:
        compositor.compose_video(
            video_path=test_video_path,
            audio_path=audio_path,
            sentence_timings=sentence_timings,
            output_path=output_path
        )

        # Verify output
        if not verify_video_dimensions(output_path):
            print("FAIL: Video dimensions are not 1080x1920")
            return False

        if not verify_audio_duration(output_path, audio_path):
            print("FAIL: Video/audio duration mismatch")
            return False

        return True

    finally:
        compositor.cleanup()
        # Clean up test files
        for f in [test_video_path, audio_path, output_path]:
            if os.path.exists(f):
                os.remove(f)


def test_memory_cleanup(brand: BrandConfig, iterations: int = 10) -> bool:
    """Test memory doesn't leak across multiple generations.

    Success Criteria #5: Memory cleanup verified across 10 consecutive generations.
    """
    tracemalloc.start()

    baseline_memory = None
    memory_samples = []

    with tempfile.TemporaryDirectory() as tmpdir:
        for i in range(iterations):
            print(f"  Memory test iteration {i + 1}/{iterations}...")

            # Force GC before measurement
            gc.collect()

            current, peak = tracemalloc.get_traced_memory()

            if baseline_memory is None:
                baseline_memory = current
            memory_samples.append(current)

            # Run a video generation cycle
            test_single_video_generation(brand, tmpdir)

            # Force GC after generation
            gc.collect()

    tracemalloc.stop()

    # Analyze memory trend
    # Memory should not grow significantly (allow 20% growth for Python overhead)
    final_memory = memory_samples[-1]
    growth_ratio = final_memory / baseline_memory if baseline_memory > 0 else 1.0

    print(f"  Baseline memory: {baseline_memory / 1024:.1f} KB")
    print(f"  Final memory: {final_memory / 1024:.1f} KB")
    print(f"  Growth ratio: {growth_ratio:.2f}x")

    # Allow up to 50% growth (1.5x) to account for Python internals
    if growth_ratio > 1.5:
        print(f"FAIL: Memory grew {growth_ratio:.2f}x (exceeds 1.5x threshold)")
        return False

    return True


def main():
    """Run all verification tests."""
    print("=" * 60)
    print("Phase 3: Core Video Composition - Verification")
    print("=" * 60)

    # Load a real brand for testing
    try:
        brand = load_brand("menopause-planner")
    except FileNotFoundError:
        # Fallback to test brand if real brand not available
        from pydantic_extra_types.color import Color
        brand = BrandConfig(
            name="Test Brand",
            slug="test-brand",
            colors=ColorPalette(
                primary=Color("#4A7C59"),
                secondary=Color("#D4A5A5")
            ),
            tts_voice="en-US-JennyNeural",
            cta_text="Test CTA",
            cta_url="https://test.com"
        )

    all_passed = True

    # Test 1: Single video generation
    print("\n[Test 1] Single video generation...")
    with tempfile.TemporaryDirectory() as tmpdir:
        if test_single_video_generation(brand, tmpdir):
            print("PASS: Video generated with correct dimensions and audio sync")
        else:
            print("FAIL: Video generation test failed")
            all_passed = False

    # Test 2: Memory cleanup across 10 iterations
    print("\n[Test 2] Memory cleanup (10 iterations)...")
    if test_memory_cleanup(brand, iterations=10):
        print("PASS: Memory stable across 10 generations")
    else:
        print("FAIL: Memory leak detected")
        all_passed = False

    # Summary
    print("\n" + "=" * 60)
    if all_passed:
        print("SUCCESS: All Phase 3 verification tests passed!")
    else:
        print("FAILURE: Some tests failed - see above for details")
    print("=" * 60)

    return 0 if all_passed else 1


if __name__ == "__main__":
    exit(main())
```
  </action>
  <verify>
```bash
python -c "
import ast
# Parse the script to verify it's valid Python
with open('scripts/test_compositor.py', 'r') as f:
    ast.parse(f.read())
print('Script is valid Python')

# Check key functions exist
from scripts.test_compositor import test_single_video_generation, test_memory_cleanup, main
print('Key test functions exist')
"
```
  </verify>
  <done>Verification script exists with tests for all success criteria</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete video composition pipeline with:
- VideoCompositor class that converts 16:9 to 9:16 vertical video
- Text overlay system with brand colors and safe zone positioning
- Audio-text synchronization using edge-tts word boundaries
- Memory cleanup to prevent leaks in batch processing
- Verification script testing all success criteria
  </what-built>
  <how-to-verify>
1. Run the verification script:
```bash
python scripts/test_compositor.py
```

2. Expected output:
- "PASS: Video generated with correct dimensions and audio sync"
- "PASS: Memory stable across 10 generations"
- "SUCCESS: All Phase 3 verification tests passed!"

3. If any tests fail, review the error messages and check:
- FFmpeg is installed with libx264 codec
- All dependencies are installed (moviepy, edge-tts, pillow)
- Network connection for edge-tts (requires internet)

4. Optionally, manually inspect a generated test video:
```bash
# Create a test video to inspect
python -c "
import tempfile
import os
from scripts.test_compositor import test_single_video_generation
from src.utils.brand_loader import load_brand
brand = load_brand('menopause-planner')
with tempfile.TemporaryDirectory() as tmpdir:
    # Modify to keep output
    print(f'Check output in: {tmpdir}')
    test_single_video_generation(brand, tmpdir)
"
```
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe any failures for debugging</resume-signal>
</task>

</tasks>

<verification>
All Phase 3 success criteria verified by the test script:

1. **Compositor accepts 16:9 and outputs 1080x1920** - Verified by `verify_video_dimensions()`
2. **Text overlays with brand colors in safe zones** - Uses create_text_overlay with brand_config
3. **Text syncs to audio within 100ms** - Uses edge-tts word boundaries (10ms precision)
4. **Audio plays without drift** - Verified by `verify_audio_duration()` with tolerance
5. **Memory cleanup across 10 generations** - Verified by `test_memory_cleanup()` tracking memory growth
</verification>

<success_criteria>
- compose_video method exists and wires all components together
- VideoCompositor exports all timing utilities
- Verification script tests all 5 success criteria from roadmap
- All tests pass when `python scripts/test_compositor.py` is run
- Memory growth stays under 1.5x across 10 iterations
- Output video is exactly 1080x1920 pixels
- Audio duration matches video duration within 100ms
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-video-composition/03-04-SUMMARY.md`
</output>
