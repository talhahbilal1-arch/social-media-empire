name: Content Engine

on:
  schedule:
    # 5 runs per day â€” staggered for Pinterest optimal times (PST)
    - cron: '0 15 * * *'  # 7 AM PST
    - cron: '0 18 * * *'  # 10 AM PST
    - cron: '0 21 * * *'  # 1 PM PST
    - cron: '0 1 * * *'   # 5 PM PST (next day UTC)
    - cron: '0 4 * * *'   # 8 PM PST (next day UTC)
  workflow_dispatch:
    inputs:
      brand:
        description: 'Specific brand to generate for (fitness/deals/menopause)'
        required: false
        type: string
      dry_run:
        description: 'Dry run (no posting)'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.11'

jobs:
  generate-and-post:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate and post pins
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PEXELS_API_KEY: ${{ secrets.PEXELS_API_KEY }}
          CREATOMATE_API_KEY: ${{ secrets.CREATOMATE_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          MAKE_WEBHOOK_DEALS: ${{ secrets.MAKE_WEBHOOK_DEALS }}
          MAKE_WEBHOOK_MENOPAUSE: ${{ secrets.MAKE_WEBHOOK_MENOPAUSE }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          python3 << 'PYEOF'
          import os
          import sys
          import json
          import requests
          import time
          from datetime import datetime, timezone

          sys.path.insert(0, '.')
          from video_automation.content_brain import (
              generate_pin_content, generate_pin_from_calendar,
              log_pin_to_history, build_destination_url
          )
          from video_automation.image_selector import get_unique_pexels_image
          from video_automation.pin_image_generator import render_pin_to_bytes, map_visual_style
          from video_automation.supabase_storage import upload_pin_image
          from video_automation.pinterest_boards import get_board_id

          dry_run = os.environ.get('DRY_RUN', 'false') == 'true'

          # Override brand if specified, else all 3 brands every run (1 pin each = 15/day)
          manual_brand = os.environ.get('BRAND', '${{ github.event.inputs.brand }}').strip()
          if manual_brand and manual_brand != '':
              brands = [b.strip() for b in manual_brand.split(',')]
          else:
              brands = ['fitness', 'deals', 'menopause']

          # Initialize Supabase
          from database.supabase_client import get_supabase_client
          db = get_supabase_client()

          def post_to_make_webhook(webhook_url, payload, brand_name, max_retries=2):
              """Post to Make.com webhook with retry logic."""
              for attempt in range(1, max_retries + 1):
                  try:
                      resp = requests.post(webhook_url, json=payload, timeout=30)
                      if resp.status_code < 400:
                          return True, f'{resp.status_code}'
                      error_msg = f'HTTP {resp.status_code}: {resp.text[:300]}'
                      print(f'  Make.com attempt {attempt}/{max_retries} failed for {brand_name}: {error_msg}')
                  except requests.exceptions.RequestException as req_err:
                      error_msg = str(req_err)
                      print(f'  Make.com attempt {attempt}/{max_retries} request error for {brand_name}: {error_msg}')

                  if attempt < max_retries:
                      print(f'  Waiting 60s before retry...')
                      time.sleep(60)

              return False, error_msg

          results = []
          for brand_idx, brand in enumerate(brands):
              brand = brand.strip()
              print(f'\n=== Generating pin for {brand} ===')
              try:
                  # Step 1: Generate content via Claude (calendar-aware with fallback)
                  pin_data = generate_pin_from_calendar(brand, db.client)
                  if pin_data is None:
                      print(f'  Calendar exhausted for today, using random topic')
                      pin_data = generate_pin_content(brand, db.client)
                  print(f'  Title: {pin_data["title"]}')
                  print(f'  Board: {pin_data["board"]}')
                  print(f'  Style: {pin_data["visual_style"]}')
                  print(f'  Text overlay: {pin_data.get("text_overlay", "N/A")}')

                  # Step 2: Get unique image from Pexels
                  image = get_unique_pexels_image(
                      pin_data['image_search_query'], brand, db.client
                  )
                  pin_data['pexels_image_id'] = image['id']
                  pin_data['image_url'] = image['url']
                  print(f'  Image: {image["id"]} by {image["photographer"]}')

                  # Step 3: Render text overlay pin with PIL
                  pil_style = map_visual_style(pin_data['visual_style'])
                  text_overlay = pin_data.get('text_overlay', pin_data['title'])
                  image_bytes = render_pin_to_bytes(
                      brand=brand,
                      headline=text_overlay,
                      subheadline=pin_data['title'],
                      keyword_or_url=image['url'],
                      style=pil_style,
                  )
                  print(f'  Rendered pin: {len(image_bytes)} bytes ({pil_style} style)')

                  # Step 4: Upload to Supabase Storage
                  timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
                  filename = f'{brand}_{timestamp}.jpg'
                  rendered_url = upload_pin_image(image_bytes, filename)
                  print(f'  Uploaded: {rendered_url}')

                  # Step 5: Build destination URL with UTM
                  posting_method = f'make_{brand}'
                  topic_slug = pin_data.get('topic', '').replace(' ', '-').replace('/', '-')[:40]
                  pin_data['destination_url'] = build_destination_url(
                      pin_data['destination_url'], brand, posting_method, 'pins',
                      topic_slug=topic_slug, board_name=pin_data.get('board', '')
                  )

                  # Step 6: Resolve board_id from board name
                  board_id = get_board_id(brand, pin_data.get('board', ''))
                  print(f'  Board ID: {board_id}')

                  if dry_run:
                      print(f'  [DRY RUN] Would post: {pin_data["title"]}')
                      results.append({'brand': brand, 'status': 'dry_run', 'title': pin_data['title']})
                      continue

                  # Step 7: Post via Make.com webhook
                  brand_map = {
                      'fitness': 'fitness-made-easy',
                      'deals': 'daily-deal-darling',
                      'menopause': 'menopause-planner',
                  }
                  webhook = os.environ.get('MAKE_WEBHOOK_DEALS', '') or os.environ.get('MAKE_WEBHOOK_MENOPAUSE', '')
                  if webhook:
                      payload = {
                          'brand': brand_map.get(brand, brand),
                          'title': pin_data['title'],
                          'description': pin_data['description'],
                          'image_url': rendered_url,
                          'board_id': board_id,
                          'link': pin_data['destination_url'],
                      }
                      success, result_msg = post_to_make_webhook(webhook, payload, brand)
                      if success:
                          pin_data['posting_method'] = posting_method
                          print(f'  Posted via Make.com ({posting_method}): {result_msg}')
                      else:
                          print(f'  Make.com webhook FAILED after retries for {brand}: {result_msg}')
                          pin_data['posting_method'] = 'failed'
                  else:
                      print(f'  WARNING: No Make.com webhook configured - skipping {brand}')
                      pin_data['posting_method'] = 'skipped'

                  # Step 8: Log to content_history
                  status_map = {'skipped': 'skipped', 'failed': 'failed'}
                  actual_status = status_map.get(pin_data.get('posting_method', ''), 'posted')
                  log_pin_to_history(pin_data, db.client)
                  results.append({'brand': brand, 'status': actual_status, 'title': pin_data['title']})
                  print(f'  Status: {actual_status} | Logged to content_history')

                  # Stagger posts 30s apart between brands
                  if actual_status == 'posted' and brand_idx < len(brands) - 1:
                      print(f'  Waiting 30s before next brand...')
                      time.sleep(30)

              except Exception as e:
                  print(f'  ERROR generating/posting for {brand}: {e}')
                  import traceback
                  traceback.print_exc()
                  try:
                      db.client.table('errors').insert({
                          'error_type': 'content_engine',
                          'error_message': str(e),
                          'context': json.dumps({'brand': brand}),
                          'severity': 'high',
                          'created_at': datetime.now(timezone.utc).isoformat()
                      }).execute()
                  except:
                      pass
                  results.append({'brand': brand, 'status': 'failed', 'error': str(e)})
                  continue

          # Summary
          print(f'\n=== Content Engine Summary ===')
          for r in results:
              status = r['status'].upper()
              print(f'  {r["brand"]}: {status} - {r.get("title", r.get("error", ""))}')

          # Update agent_runs
          posted = [r for r in results if r['status'] == 'posted']
          failed = [r for r in results if r['status'] == 'failed']
          skipped = [r for r in results if r['status'] == 'skipped']
          if skipped:
              print(f'\n  WARNING: {len(skipped)} brand(s) skipped due to missing config')
          if failed:
              print(f'  WARNING: {len(failed)} brand(s) failed to post')
          agents_to_update = ['content_brain', 'content_pipeline', 'image_selector']
          if posted:
              agents_to_update.append('multi_platform_poster')
          for agent in agents_to_update:
              try:
                  db.client.table('agent_runs').upsert({
                      'agent_name': agent,
                      'last_run_at': datetime.now(timezone.utc).isoformat(),
                      'status': 'success' if posted else 'failed',
                      'updated_at': datetime.now(timezone.utc).isoformat()
                  }, on_conflict='agent_name').execute()
              except:
                  pass

          # Fail the workflow only if ALL brands failed
          non_failed = [r for r in results if r['status'] != 'failed']
          if not non_failed:
              sys.exit(1)
          PYEOF

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: content-engine-${{ github.run_number }}
          path: '*.log'
          retention-days: 7
