name: Content Engine

on:
  schedule:
    # 3 runs per day — staggered 5+ hours apart for Pinterest optimal times
    - cron: '0 15 * * *'  # 7 AM PST (early morning scroll)
    - cron: '0 21 * * *'  # 1 PM PST (lunch break)
    - cron: '0 3 * * *'   # 7 PM PST (evening scroll)
  workflow_dispatch:
    inputs:
      brand:
        description: 'Specific brand to generate for (fitness/deals/menopause)'
        required: false
        type: string
      dry_run:
        description: 'Dry run (no posting)'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.11'

jobs:
  generate-and-post:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Determine posting slot
        id: slot
        run: |
          # Determine which slot this is based on cron schedule
          HOUR=$(date -u +%H)
          if [ "$HOUR" -ge "14" ] && [ "$HOUR" -le "16" ]; then
            SLOT="morning"
            BRANDS="fitness,deals,menopause"
          elif [ "$HOUR" -ge "20" ] && [ "$HOUR" -le "22" ]; then
            SLOT="afternoon"
            BRANDS="fitness,deals"
          else
            SLOT="evening"
            BRANDS="fitness,menopause"
          fi

          # Override if manual brand specified
          if [ -n "${{ github.event.inputs.brand }}" ]; then
            BRANDS="${{ github.event.inputs.brand }}"
          fi

          echo "slot=$SLOT" >> $GITHUB_OUTPUT
          echo "brands=$BRANDS" >> $GITHUB_OUTPUT
          echo "Slot: $SLOT | Brands: $BRANDS"

      - name: Generate and post pins
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PEXELS_API_KEY: ${{ secrets.PEXELS_API_KEY }}
          CREATOMATE_API_KEY: ${{ secrets.CREATOMATE_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          MAKE_WEBHOOK_DEALS: ${{ secrets.MAKE_WEBHOOK_DEALS }}
          MAKE_WEBHOOK_MENOPAUSE: ${{ secrets.MAKE_WEBHOOK_MENOPAUSE }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          python -c "
          import os
          import sys
          import json
          import requests
          import time
          from datetime import datetime

          sys.path.insert(0, '.')
          from video_automation.content_brain import generate_pin_content, generate_pin_from_calendar, log_pin_to_history
          from video_automation.image_selector import get_unique_pexels_image

          dry_run = os.environ.get('DRY_RUN', 'false') == 'true'
          brands = os.environ.get('BRANDS', '${{ steps.slot.outputs.brands }}').split(',')
          slot = '${{ steps.slot.outputs.slot }}'

          # Initialize Supabase
          from database.supabase_client import get_supabase_client
          db = get_supabase_client()

          def post_to_make_webhook(webhook_url, payload, brand_name, max_retries=2):
              \"\"\"Post to Make.com webhook with retry logic.

              Tries up to max_retries times with a 60s wait between attempts.
              Returns (success: bool, response_or_error: str).
              \"\"\"
              for attempt in range(1, max_retries + 1):
                  try:
                      resp = requests.post(webhook_url, json=payload, timeout=30)
                      if resp.status_code < 400:
                          return True, f'{resp.status_code}'
                      error_msg = f'HTTP {resp.status_code}: {resp.text[:300]}'
                      print(f'  Make.com attempt {attempt}/{max_retries} failed for {brand_name}: {error_msg}')
                  except requests.exceptions.RequestException as req_err:
                      error_msg = str(req_err)
                      print(f'  Make.com attempt {attempt}/{max_retries} request error for {brand_name}: {error_msg}')

                  if attempt < max_retries:
                      print(f'  Waiting 60s before retry...')
                      time.sleep(60)

              return False, error_msg

          results = []
          for brand_idx, brand in enumerate(brands):
              brand = brand.strip()
              print(f'\n=== Generating pin for {brand} ===')
              try:
                  # Step 1: Generate content via Claude (calendar-aware with fallback)
                  pin_data = generate_pin_from_calendar(brand, db.client)
                  if pin_data is None:
                      print(f'  Calendar exhausted for today, using random topic')
                      pin_data = generate_pin_content(brand, db.client)
                  print(f'  Title: {pin_data[\"title\"]}')
                  print(f'  Board: {pin_data[\"board\"]}')
                  print(f'  Style: {pin_data[\"visual_style\"]}')

                  # Step 2: Get unique image
                  image = get_unique_pexels_image(
                      pin_data['image_search_query'], brand, db.client
                  )
                  pin_data['pexels_image_id'] = image['id']
                  pin_data['image_url'] = image['url']
                  print(f'  Image: {image[\"id\"]} by {image[\"photographer\"]}')

                  # Step 3: Build destination URL with UTM
                  # ALL brands now route through Make.com (no Late API)
                  from video_automation.content_brain import build_destination_url
                  posting_method = f'make_{brand}'
                  # Generate topic slug for pin-level UTM tracking
                  topic_slug = pin_data.get('topic', '').replace(' ', '-').replace('/', '-')[:40]
                  pin_data['destination_url'] = build_destination_url(
                      pin_data['destination_url'], brand, posting_method, slot,
                      topic_slug=topic_slug, board_name=pin_data.get('board', '')
                  )

                  if dry_run:
                      print(f'  [DRY RUN] Would post: {pin_data[\"title\"]}')
                      results.append({'brand': brand, 'status': 'dry_run', 'title': pin_data['title']})
                      continue

                  # Step 4: Post via Make.com webhook (ALL brands)
                  # The Make.com scenario routes to the correct Pinterest account
                  # based on the brand field. No Late API — all traffic goes through Make.com.
                  brand_map = {
                      'fitness': 'fitness-made-easy',
                      'deals': 'daily-deal-darling',
                      'menopause': 'menopause-planner',
                  }
                  # Use MAKE_WEBHOOK_DEALS as the shared webhook (all brands use same scenario)
                  webhook = os.environ.get('MAKE_WEBHOOK_DEALS', '') or os.environ.get('MAKE_WEBHOOK_MENOPAUSE', '')
                  if webhook:
                      payload = {
                          'brand': brand_map.get(brand, brand),
                          'title': pin_data['title'],
                          'description': pin_data['description'],
                          'image_url': pin_data['image_url'],
                          'board_name': pin_data.get('board', ''),
                          'link': pin_data['destination_url'],
                      }
                      success, result_msg = post_to_make_webhook(webhook, payload, brand)
                      if success:
                          method_name = f'make_{brand}'
                          pin_data['posting_method'] = method_name
                          print(f'  Posted via Make.com ({method_name}): {result_msg}')
                      else:
                          print(f'  Make.com webhook FAILED after retries for {brand}: {result_msg}')
                          pin_data['posting_method'] = 'failed'
                  else:
                      print(f'  WARNING: No Make.com webhook configured - skipping {brand}')
                      pin_data['posting_method'] = 'skipped'

                  # Step 5: Log to content_history and track result correctly
                  status_map = {'skipped': 'skipped', 'failed': 'failed'}
                  actual_status = status_map.get(pin_data.get('posting_method', ''), 'posted')
                  log_pin_to_history(pin_data, db.client)
                  results.append({'brand': brand, 'status': actual_status, 'title': pin_data['title']})
                  print(f'  Status: {actual_status} | Logged to content_history')

                  # Stagger posts 30s apart between brands to avoid flooding
                  if actual_status == 'posted' and brand_idx < len(brands) - 1:
                      print(f'  Waiting 30s before next brand...')
                      time.sleep(30)

              except Exception as e:
                  print(f'  ERROR generating/posting for {brand}: {e}')
                  # Log error to Supabase
                  try:
                      db.client.table('errors').insert({
                          'error_type': 'content_engine',
                          'error_message': str(e),
                          'context': json.dumps({'brand': brand, 'slot': slot}),
                          'severity': 'high',
                          'created_at': datetime.utcnow().isoformat()
                      }).execute()
                  except:
                      pass
                  results.append({'brand': brand, 'status': 'failed', 'error': str(e)})
                  continue  # Don't let one failure kill the whole run

          # Summary
          print(f'\n=== Content Engine Summary ===')
          for r in results:
              status = r['status'].upper()
              print(f'  {r[\"brand\"]}: {status} - {r.get(\"title\", r.get(\"error\", \"\"))}')

          # Update agent_runs so monitoring knows we're alive
          posted = [r for r in results if r['status'] == 'posted']
          failed = [r for r in results if r['status'] == 'failed']
          skipped = [r for r in results if r['status'] == 'skipped']
          if skipped:
              print(f'\n  WARNING: {len(skipped)} brand(s) skipped due to missing config')
          if failed:
              print(f'  WARNING: {len(failed)} brand(s) failed to post')
          agents_to_update = ['content_brain', 'content_pipeline', 'image_selector']
          if posted:
              agents_to_update.append('multi_platform_poster')
          for agent in agents_to_update:
              try:
                  db.client.table('agent_runs').upsert({
                      'agent_name': agent,
                      'last_run_at': datetime.utcnow().isoformat(),
                      'status': 'success' if posted else 'failed',
                      'updated_at': datetime.utcnow().isoformat()
                  }, on_conflict='agent_name').execute()
              except:
                  pass  # Table may not exist yet

          # Fail the workflow only if ALL brands failed (skipped counts as non-failure)
          non_failed = [r for r in results if r['status'] != 'failed']
          if not non_failed:
              sys.exit(1)
          "

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: content-engine-${{ steps.slot.outputs.slot }}-${{ github.run_number }}
          path: '*.log'
          retention-days: 7
